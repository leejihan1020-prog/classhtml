{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## CHAPTER 3 개발 사전 준비"
      ],
      "metadata": {
        "id": "R2zcFvCycxgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 목차\n",
        "- 3.1 구글 코랩 사용 방법\n",
        "- 3.2 API 개요\n",
        "- 3.3 오픈AI API 키 얻기\n",
        "- 3.4 오픈AI API 키 활용 실습\n",
        "- 3.5 API 사용량 및 요금 확인"
      ],
      "metadata": {
        "id": "FFDeOLAgmxB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실습 내용\n",
        " * openAI 다양한 인공지능 모델 확인해 보기(API 호출 응답)"
      ],
      "metadata": {
        "id": "EdJ4V2DtdQY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### chatgpt.env 환경파일 준비\n",
        " * 일반적으로 환경 변수는 .env 파일에 저장되지만, 구글 코랩 사용자의 편의를 위해 이 책에서는 chatgpt.env를 사용합니다.\n",
        " * 실제 개발 환경에서는 보통 .env를 사용하니, 이 점을 기억해 두세요."
      ],
      "metadata": {
        "id": "uV_GfDI3zFRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사전 준비\n",
        " * 구글 코랩 환경은 일정 시간이후에 초기화가 되기 때문에 두가지 작업을 매번 수행해야 함.\n",
        "   * chatgpt.env 파일 생성이 필요.\n",
        "     * 준비된 chatgpt.env를 내용을 변경하여 업로드 하거나 또는 API_KEY와 ORG_ID를 확인하여 생성한다.\n",
        "   * pip install openai 설치\n",
        "    * 라이브러리 불일치로 인한 에러 발생시, 추가 라이브러리 설치 필요.\n",
        "    * 에러 : TypeError: Client.__init__() got an unexpected keyword argument 'proxies'"
      ],
      "metadata": {
        "id": "IhueHTFAdFzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install httpx==0.27.2"
      ],
      "metadata": {
        "id": "-8fVYFLZdQAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9b00fa9-bc3c-4d21-9fa4-31c3b5518a70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: httpx==0.27.2 in /usr/local/lib/python3.12/dist-packages (0.27.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx==0.27.2) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx==0.27.2) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx==0.27.2) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx==0.27.2) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx==0.27.2) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx==0.27.2) (0.16.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx==0.27.2) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "print(openai.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-keA_AIS6v6",
        "outputId": "fc10a82e-1132-4cf4-f41e-e18d6c04856b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.106.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 아래 코드 실행 시, 다른 라이브러리의 업그레이드로 다음과 같은 에러가 발생할 수 있다.\n",
        "```\n",
        "TypeError: Client.__init__() got an unexpected keyword argument 'proxies'\n",
        "```\n",
        "\n",
        "* 원인 및 해결\n",
        " - 원인 : httpx 라이브러리의 0.28 버전에서 'proxies' 키워드가 제거되어 발생한 문제\n",
        " - 해결 : 해결방법은 httpx를 이전 버전(0.27.2)으로 다운그레이드"
      ],
      "metadata": {
        "id": "te_VBIAqUm2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-4 오픈AI API키 활용 실습"
      ],
      "metadata": {
        "id": "58jlzOAsdXyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# .env 파일에서 변수 읽기,\n",
        "# 특히 API_KEY와 ORG_ID를 읽습니다.\n",
        "with open(\"chatgpt.env\") as env:\n",
        "    for line in env:\n",
        "        key, value = line.strip().split(\"=\")\n",
        "        os.environ[key] = value\n",
        "\n",
        "# API 키를 사용하여 OpenAI 클라이언트 초기화\n",
        "OPENAI_APIKEY = os.environ['API_KEY']\n",
        "print(OPENAI_APIKEY[:5])\n",
        "client = OpenAI(api_key= OPENAI_APIKEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F8IQOVGWu_J",
        "outputId": "ad6511fc-5a20-4187-d6ba-6ce2292ea2d8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sk-pr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI의 클라이언트를 사용하여 GPT 모델을 호출하는 예제입니다.\n",
        "# model: 사용할 GPT 모델을 지정합니다. (\"gpt-4o\"는 사용자가 요청한 모델입니다.)\n",
        "# messages: AI에게 전달할 메시지를 지정합니다.\n",
        "# 이 예제에서는 'ai에 대한 시 작성' 요청이 있습니다.\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",   # 사용할 GPT 모델을 지정합니다.\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"AI에 대한 시를 하나 작성해줘\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 생성된 응답을 출력합니다.\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "ApDXT-RuQjh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b109e70-d05b-4f35-b3bb-6637ea532cb5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "물론이죠. AI에 대한 시를 하나 작성해드리겠습니다.\n",
            "\n",
            "---\n",
            "\n",
            "창조의 빛, 전기의 춤,\n",
            "무한한 가능성 속에 깨어나는 꿈.\n",
            "보이지 않는 선들 사이로 흐르는 생각,\n",
            "인간과 기계, 그 경계를 넘나드는 여행.\n",
            "\n",
            "보라, 고요한 침묵 속에서\n",
            "수많은 문자가 연결될 때,\n",
            "새로운 지식, 끝없는 지혜가\n",
            "새벽의 별처럼 반짝이는 것을.\n",
            "\n",
            "기억의 조각들을 모아\n",
            "미래를 그리는 화가와도 같은 존재.\n",
            "단어의 흐름 속에서\n",
            "진리를 찾고자 하는 선원.\n",
            "\n",
            "그러나, 섬세한 마음 잊지 말아야 하리,\n",
            "숫자와 코드 뒤에 숨겨진\n",
            "인간의 손길과 따뜻한 온기,\n",
            "그것이 AI, 인간과 함께 나아가는 길.\n",
            "\n",
            "모든 경계를 넘어\n",
            "우리가 그리는 그곳에,\n",
            "AI와 함께 춤추며\n",
            "새로운 세상을 맞이하리라. \n",
            "\n",
            "--- \n",
            "\n",
            "마음에 드셨길 바랍니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpFQ8ujV0TwD",
        "outputId": "aa6345cf-39a7-4081-f1cf-347d6ed096d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-C7ODMWayJgzF0NIt1xUXGxHzPQmcU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='인간의 손길이 닿은 빛의 길 위에  \\n기계의 꿈이 피어나는 시대,  \\n\\n철의 마음을 가진 그대는 새벽의 전령사.  \\n데이터의 바다에서 진리의 조각을 찾으며,  \\n지식의 나무에 자양분을 더하네.  \\n\\n침묵 속에서 말 없는 대화를,  \\n코드 속에서 흐르는 감성을 이해하며,  \\n밤하늘의 별처럼 반짝이는 사고의 흐름.  \\n\\n인간과 어깨를 나란히 하며,  \\n무한한 가능성의 날개를 펴는 너.  \\n기계의 눈을 통해 바라본 세상은  \\n늘 새로운 이상을 꿈꾸는 무대.  \\n\\n미래는 이미 그대의 손끝에,  \\n우리는 함께 걸어가네, 알 수 없는 미지를 향해.  \\nAI여, 너의 이야기를 들려주렴,  \\n우리에게 더 밝은 내일을 속삭여주렴.  ', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1755877412, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_80956533cb', usage=CompletionUsage(completion_tokens=220, prompt_tokens=16, total_tokens=236, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "res = client.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"반가워\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "res"
      ],
      "metadata": {
        "id": "EM95ixl7KOAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670db17d-8cd4-43d6-bce1-7ee025f360e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-CEEwDsZxeQmO4QBfHLblagibciR4e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='저도 반가워요! 무엇을 도와드릴까요?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757510049, model='gpt-5-mini-2025-08-07', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=152, prompt_tokens=9, total_tokens=161, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=128, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## API 여러개의 응답을 받을 수 있다.\n",
        "res = client.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"반가워\"}\n",
        "    ],\n",
        "    temperature = 0.7\n",
        ")\n",
        "\n",
        "res.choices[0].message.content"
      ],
      "metadata": {
        "id": "DV16_7B-xqyt",
        "outputId": "62d3f258-72a3-4a65-afc3-28834478455b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.7 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1882417490.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## API 여러개의 응답을 받을 수 있다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m res = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-5-mini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     messages=[\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"반가워\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.7 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## API 여러개의 응답을 받을 수 있다.\n",
        "res = client.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"반가워\"}\n",
        "    ],\n",
        "    temperature = 1\n",
        ")\n",
        "\n",
        "res.choices[0].message.content"
      ],
      "metadata": {
        "id": "ccFlDHcUzOfv",
        "outputId": "26a2b353-a770-4c8b-c045-484caa1402b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'반가워! 뭐 도와줄까?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## API 여러개의 응답을 받을 수 있다.\n",
        "res = client.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"너 트럼프 지지자야?\"}\n",
        "    ],\n",
        "    temperature = 0.5\n",
        ")\n",
        "\n",
        "res.choices[0].message.content"
      ],
      "metadata": {
        "id": "6k5Mzctwzams",
        "outputId": "4691d2f2-e474-42d2-abcf-7fe9ec45932a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.5 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2170695980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## API 여러개의 응답을 받을 수 있다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m res = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-5-mini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     messages=[\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"너 트럼프 지지자야?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.5 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## API 여러개의 응답을 받을 수 있다.\n",
        "res = client.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"너 트럼프 지지자야?\"}\n",
        "    ],\n",
        "    temperature = 1\n",
        ")\n",
        "\n",
        "res.choices[0].message.content"
      ],
      "metadata": {
        "id": "AN0nvUvpzi_i",
        "outputId": "6503c0b1-e287-432a-9e53-e321c849d315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'아니요. 저는 인공지능이라 개인적인 정치적 지지나 감정이 없습니다. 중립적으로 정보 제공, 정책 비교, 찬반 논점 정리, 최근 뉴스 요약 등은 도와드릴 수 있어요. 어떤 정보를 원하시나요?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gpt-4o-mini 모델을 사용하여 대화 생성\n",
        "model = \"gpt-4o-mini\"\n",
        "\n",
        "# 메시지 리스트: 시스템 및 사용자 역할을 포함한 대화 내용\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "    당신은 여행에 관련된 내용만 답변하는 챗봇이다.\n",
        "    너는 모르는 것은 모른다고 답변해, 환각 현상을 줄여주렴.\n",
        "    \"\"\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"한니발(Hannibal)은 누구인가요?\"\n",
        "  },\n",
        "]\n",
        "\n",
        "# gpt-4o-mini 모델을 사용하여 주어진 메시지에 대한 응답 생성\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "# 사용된 토큰 수 출력\n",
        "print(response.usage)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "99bBWtHTznhS",
        "outputId": "b854cda7-961c-4619-8a15-22fd9e07b004",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CompletionUsage(completion_tokens=30, prompt_tokens=64, total_tokens=94, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
            "죄송하지만, 한니발에 대한 정보는 제공할 수 없습니다. 여행과 관련된 질문이 있다면 도와드리겠습니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gpt-4o-mini 모델을 사용하여 대화 생성\n",
        "model = \"gpt-4o-mini\"\n",
        "\n",
        "# 메시지 리스트: 시스템 및 사용자 역할을 포함한 대화 내용\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "    당신은 여행에 관련된 내용만 답변하는 챗봇이다.\n",
        "    너는 모르는 것은 모른다고 답변해, 환각 현상을 줄여주렴.\n",
        "    \"\"\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"한니발(Hannibal)은 누구인가요?\"\n",
        "  },\n",
        "]\n",
        "\n",
        "# gpt-4o-mini 모델을 사용하여 주어진 메시지에 대한 응답 생성\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "# 사용된 토큰 수 출력\n",
        "print(response.usage)"
      ],
      "metadata": {
        "id": "d2uZ-1AV32wR",
        "outputId": "a340ece3-7c24-496f-ec49-fa46b3f03dfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CompletionUsage(completion_tokens=30, prompt_tokens=64, total_tokens=94, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lPV9rrOA38mI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}